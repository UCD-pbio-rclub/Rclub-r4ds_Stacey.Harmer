---
title: "Rclub_Sept20"
author: "Stacey Harmer"
date: "9/13/2017"
output: 
  html_document: 
    keep_md: yes
---

Sept 20: Chapter 22 (it is short) + Chapter 23.1 - 23.2

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 22 Intro to models

pair of ideas that you must understand in order to do inference correctly:

Each observation can either be used for exploration or confirmation, not both.

You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration.

# 23  Model Basics

```{r}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```

# 23.2  A simple model

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point()
```

```{r}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)
```

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()
```

we first turn our model family into an R function. This takes the model parameters and the data as inputs, and gives values predicted by the model as output:

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)

```


```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)
#> [1] 2.67
```

now expand to the big list of coeffs
```{r}
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models

```

overlay 10 best models on the data

```{r}

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(models, rank(dist) <= 10)
  )


ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(models, rank(dist) <= 3)
  )
```

Or look at model parameters

```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))

ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 2), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))

```

Or, start with a grid search

```{r}
grid <- expand.grid(
  a1 = seq(-5, 0, length = 5),
  a2 = seq(1, 2, length = 5)
  )

grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 


```

back to dataset

```{r}
  ggplot(sim1, aes(x, y)) + 
    geom_point(size = 2, colour = "grey30") + 
    geom_abline(
      aes(intercept = a1, slope = a2, colour = -dist), 
      data = filter(grid, rank(dist) <= 10)
    )
```

better way to optimum model:

```{r}
best <- optim(c(0, 0), measure_distance, data = sim1) 
head(best)

best$par  
#> [1] 4.22 2.05

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

linear model

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
summary(sim1_mod)
 
```

## 23.2.1  Exercises

1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)

# model fit
sim_mod_1a <- lm(y ~ x, data = sim1a)
sim_mod_1a[[1]][1]

ggplot(sim1a, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = sim_mod_1a[[1]][1], slope = sim_mod_1a[[1]][2])

```

had to run a few times till I got a version that is really weird.
here, one outlier seemed to change entire slope of line

Keep my last, weird dataset
```{r}
sim_odd <- sim1a 

ggplot(sim_odd, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = sim_mod_1a[[1]][1], slope = sim_mod_1a[[1]][2])

best_odd <- optim(c(0, 0), measure_distance, data = sim_odd)
best_odd$par

ggplot(sim_odd, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best_odd$par[1], slope = best_odd$par[2])

# these plots are pretty identical

```


2.  One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}

make_prediction <- function(a, data){
   a[1] + data$x * a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}

```

Use optim() to fit this model to the simulated data above and compare it to the linear model.

```{r}
optim_mean_ab_dist <- optim(c(0, 0), measure_distance, data = sim_odd)
optim_mean_ab_dist$par

ggplot(sim_odd, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = optim_mean_ab_dist$par[1], slope = optim_mean_ab_dist$par[2])

```

Yes, that looks much better.

3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optima. What’s the problem with optimising a three parameter model like this?

```{r}

model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```

I would guess that with 3 parameters there are many ways to optimize, and that solutions with similar optimization metrics may have very different parameter values.

(that is, as long as a1 and a3 equal same value, will fit equivalently)
